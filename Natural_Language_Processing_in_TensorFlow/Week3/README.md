# Sequence models

In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one. But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is the opposite of 'fun'. This week you'll start digging into a variety of model formats that are used in training models to understand context in sequence!

## Sequence models

- [Video - A conversation with Andrew Ng](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/O1bvl/a-conversation-with-andrew-ng)

- [Video - Introduction](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/09WN5/introduction)

- [Reading - Link to Andrew's sequence modeling course](https://www.coursera.org/learn/natural-language-processing-tensorflow/supplement/1sxLT/link-to-andrews-sequence-modeling-course)

- [Video - LSTMs](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/JZyMO/lstms)

- [Reading - More info on LSTMs](https://www.coursera.org/learn/natural-language-processing-tensorflow/supplement/0wa7l/more-info-on-lstms)

- [Video - Implementing LSTMs in code](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/AXwJI/implementing-lstms-in-code)

- [Lab - Check out the code! (Lab 1)](./Labs/C3_W3_Lab_1_single_layer_LSTM.ipynb)

- [Lab - Check out the code! (Lab 2)](./Labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb)

- [Video - Accuracy and loss](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/5svp6/accuracy-and-loss)

- [Video - A word from Laurence](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/ZR6bx/a-word-from-laurence)

- [Video - Looking into the code](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/TSKTU/looking-into-the-code)

- [Video - Using a convolutional network](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/fSE8o/using-a-convolutional-network)

- [Lab - Check out the code! (Lab 3)](./Labs/C3_W3_Lab_3_Conv1D.ipynb)

- [Video - Going back to the IMDB dataset](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/NFvFd/going-back-to-the-imdb-dataset)

- [Lab - Check out the code! (Lab 4)](./Labs/C3_W3_Lab_4_imdb_reviews_with_GRU_LSTM_Conv1D.ipynb)

- [Video - Tips from Laurence](https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/LyiH6/tips-from-laurence)

- [Lab - Exploring different sequence models (Lab 5)](./Labs/C3_W3_Lab_5_sarcasm_with_bi_LSTM.ipynb)

- [Lab - Exploring different sequence models (Lab 6)](./Labs/C3_W3_Lab_6_sarcasm_with_1D_convolutional.ipynb)

- [Reading - Week 3 Wrap up](https://www.coursera.org/learn/natural-language-processing-tensorflow/supplement/gcx8c/week-3-wrap-up)

## Lecture Notes (Optional)

- [Reading - Lecture Notes Week 3](./Readings/C3_W3.pdf)

## Weekly Assignment - Exploring overfitting in NLP

- [Lab - Exploring overfitting in NLP](./Labs/C3W3_Assignment.ipynb)